{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Science Bowl 2019\n\n# Introduction\n\nPBS KIDS, a trusted name in early childhood education for decades, aims to gain insights into how media can help children learn important skills for success in school and life. In this challenge, you’ll use anonymous gameplay data, including knowledge of videos watched and games played, from the PBS KIDS Measure Up! app, a game-based learning tool developed as a part of the CPB-PBS Ready To Learn Initiative with funding from the U.S. Department of Education. Competitors will be challenged to predict scores on in-game assessments and create an algorithm that will lead to better-designed games and improved learning outcomes. Your solutions will aid in discovering important relationships between engagement with high-quality educational media and learning processes.\n\n**Where does the data for the competition come from?**\nThe data used in this competition is anonymous, tabular data of interactions with the PBS KIDS Measure Up! app. Select data, such as a user’s in-app assessment score or their path through the game, is collected by the PBS KIDS Measure Up! app, a game-based learning tool.\n\n**What is the PBS KIDS Measure Up! app?**\nIn the PBS KIDS Measure Up! app, children ages 3 to 5 learn early STEM concepts focused on length, width, capacity, and weight while going on an adventure through Treetop City, Magma Peak, and Crystal Caves. Joined by their favorite PBS KIDS characters, children can also collect rewards and unlock digital toys as they play. \n\nBesides the info provided above by Kaggle, I found the following additional info on the website of the app:\n\nSpecific features of Measure Up! include:\n\n* 19 unique measuring games.\n* 10 measurement-focused video clips.\n* Sticker books featuring favorite PBS KIDS characters.\n* Rewards for completion of tasks.\n* Embedded challenges and reports to help parents and caregivers monitor kids’ progress.\n* Ability to track your child's progress using the PBS KIDS Super Vision companion app.\n\n**Evaluation**\nSubmissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.\n\nThe outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n3: the assessment was solved on the first attempt\n\n2: the assessment was solved on the second attempt\n\n1: the assessment was solved after 3 or more attempts\n\n0: the assessment was never solved\n\n\n\nFor each installation_id represented in the test set, you must predict the accuracy_group of the last assessment for that installation_id.\n\nNote that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n\nThe file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set. Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. If the attempt was correct, it contains \"correct\":true."},{"metadata":{},"cell_type":"markdown","source":"# Table of contents\n\n* [1. Understanding the train data](#1.-Understanding-the-train-data)\n* [2. Understanding the test set](#2.-Understanding-the-test-set)\n* [3. Understanding and visualizing the train labels](#3.-Understanding-and-visualizing-the train-labels)\n* [4. Feature engineering and visualizations](#4.-Feature-engineering-and-visualizations)"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nimport matplotlib.pylab as plt\nimport calendar\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain = pd.read_csv('../input/data-science-bowl-2019/train.csv')\ntrain_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\ntest = pd.read_csv('../input/data-science-bowl-2019/test.csv')\nspecs = pd.read_csv('../input/data-science-bowl-2019/specs.csv')\nsample_sub = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Understanding the train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have 11 million rows and just 11 columns. However, Kaggle provided the following note: Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n\nAs there is no point in keeping training data that cannot be used for training anyway, I am getting rid of the installation_ids that never took an assessment\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"keep_id = train[train.type == \"Assessment\"][['installation_id']].drop_duplicates()\ntrain = pd.merge(train, keep_id, on=\"installation_id\", how=\"inner\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, we have now lost about 3 million rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of unique installations in our \"smaller\" train set is now 4242."},{"metadata":{"trusted":true},"cell_type":"code","source":"keep_id.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will first visualize some of the existing columns."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.rcParams.update({'font.size': 16})\n\nfig = plt.figure(figsize=(12,10))\nax1 = fig.add_subplot(211)\nax1 = sns.countplot(y=\"type\", data=train, color=\"blue\", order = train.type.value_counts().index)\nplt.title(\"number of events by type\")\n\nax2 = fig.add_subplot(212)\nax2 = sns.countplot(y=\"world\", data=train, color=\"blue\", order = train.world.value_counts().index)\nplt.title(\"number of events by world\")\n\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will now add some new columns based on the timestamp, and visualize these."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def get_time(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    return df\n    \ntrain = get_time(train)\n\n#list(set(train['title'].unique()).union(set(test['title'].unique())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below your see the counts by date. By the way, I have wasted a lot of time on trying to fix the weird ticks on the x-axis, but this seems a bug: https://github.com/matplotlib/matplotlib/issues/13183"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,10))\nse = train.groupby('date')['date'].count()\nse.plot()\nplt.title(\"Event counts by date\")\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When looking at the day of the week, we see no major difference. Of course, we are talking about kids who don't have to go to work ;-)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,10))\nse = train.groupby('dayofweek')['dayofweek'].count()\nse.index = list(calendar.day_abbr)\nse.plot.bar()\nplt.title(\"Event counts by day of week\")\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When looking at the numbers by hour of the day, I find the distribution a little bit strange. Kids seem up late at night and don't do much early in the morning. Has this something to do with time zones perhaps?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,10))\nse = train.groupby('hour')['hour'].count()\n#se.index = list(calendar.day_abbr)\nse.plot.bar()\nplt.title(\"Event counts by hour of day\")\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Understanding the test set\n\nFrom Kaggle: For each installation_id represented in the test set, you must predict the accuracy_group of the last assessment for that installation_id."},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.installation_id.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have 1.1 million rows on a thousand unique installation_ids in the test set. Below, you can see that we have this same amount of rows in the sample submission. This means that there are no installation_ids without assessment in the test set indeed."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Understanding and visualizing the train labels"},{"metadata":{},"cell_type":"markdown","source":"The outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n3: the assessment was solved on the first attempt\n\n2: the assessment was solved on the second attempt\n\n1: the assessment was solved after 3 or more attempts\n\n0: the assessment was never solved\n\n\nI started by visualizing some of these columns"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.rcParams.update({'font.size': 22})\n\nplt.figure(figsize=(12,6))\nsns.countplot(y=\"title\", data=train_labels, color=\"blue\", order = train_labels.title.value_counts().index)\nplt.title(\"Counts of titles\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.rcParams.update({'font.size': 16})\n\nplt.figure(figsize=(8,8))\nsns.countplot(x=\"accuracy_group\", data=train_labels, color=\"blue\")\nplt.title(\"Counts of accuracy group\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the match between the train dataframe and the train_labels dataframe is not straightforward, it tried to figure out how these dataframes are to be matched by focussing on just one particular installation_id."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels[train_labels.installation_id == \"0006a69f\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From Kaggle: The file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set. Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. If the attempt was correct, it contains \"correct\":true.\n\nHowever, in the first version I already noticed that I had one attempt too many for this installation_id when mapping the rows with the train_labels for. It turns out that there are in fact also assessment attemps for Bird Measurer with event_code 4100, which should not count (see below). In this case that also makes sense as this installation_id already had a pass on the first attempt"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[(train.event_code == 4100) & (train.installation_id == \"0006a69f\") & (train.title == \"Bird Measurer (Assessment)\")]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After having read Andrew Lukyanenko's notebook, I decided to also create an attempt variable. In addition, I have also borrowed his code to extract the correct variable (thanks Andrew!)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#credits for this code chuck go to Andrew Lukyanenko\ntrain['attempt'] = 0\ntrain.loc[(train['title'] == 'Bird Measurer (Assessment)') & (train['event_code'] == 4110),\\\n       'attempt'] = 1\ntrain.loc[(train['type'] == 'Assessment') &\\\n       (train['title'] != 'Bird Measurer (Assessment)')\\\n       & (train['event_code'] == 4100),\\\n          'attempt'] = 1\n\ntrain['correct'] = None\ntrain.loc[(train['attempt'] == 1) & (train['event_data'].str.contains('\"correct\":true')), 'correct'] = True\ntrain.loc[(train['attempt'] == 1) & (train['event_data'].str.contains('\"correct\":false')), 'correct'] = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below you can see that row 2615 is not in the assessment results of this installation_id anymore, and the results add up nicely to the num_correct and num_incorrect in the train_labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[(train.installation_id == \"0006a69f\") & (train.attempt == 1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the question arises: Could there be installation_id's who did assessments (we have already taken out the ones who never took one), but without results in the train_labels? As you can see below, yes there are 628 of those."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[~train.installation_id.isin(train_labels.installation_id.unique())].installation_id.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can not train on those installation_id's anyway, I am taking them out of the train set. This reduces our train set further from 8.3 million rows to 7.7 million."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train.installation_id.isin(train_labels.installation_id.unique())]\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Feature engineering; adding features to train_labels"},{"metadata":{},"cell_type":"markdown","source":"Basically what we need to do is add features to the train_labels dataframe to get our training dataset. To be continued in the next version. Please stay tuned!"},{"metadata":{},"cell_type":"markdown","source":"**To be continued**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}